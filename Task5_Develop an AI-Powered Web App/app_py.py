# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18CPg91JVrGReEq4gX8ONci4bgYdlHZ1z
"""

!pip install streamlit tensorflow pillow pyngrok -q

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np

print(" All packages installed successfully!")

def create_transfer_learning_model():
    print("üèóÔ∏è Creating Transfer Learning Model...")

    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )

    base_model.trainable = False

    inputs = base_model.input
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.2)(x)

    outputs = Dense(5, activation='softmax', name='predictions')(x)

    model = Model(inputs, outputs)

    model.compile(
        optimizer=Adam(learning_rate=0.0001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print("‚úÖ Model created successfully!")
    print(f"üìä Model has {model.count_params():,} parameters")

    model.save("custom_model.h5")
    print("üíæ Model saved as 'custom_model.h5'")

    return model

model = create_transfer_learning_model()

streamlit_app_code = '''import streamlit as st
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import numpy as np
from PIL import Image
import io

st.set_page_config(
    page_title="ü§ñ AI Image Classifier",
    page_icon="üñºÔ∏è",
    layout="wide"
)

st.markdown(
    "<style>"
    ".main-header {"
    "    font-size: 3.5rem;"
    "    color: #1e88e5;"
    "    text-align: center;"
    "    margin-bottom: 2rem;"
    "    text-shadow: 2px 2px 4px rgba(0,0,0,0.1);"
    "}"
    ".prediction-box {"
    "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);"
    "    padding: 25px;"
    "    border-radius: 15px;"
    "    color: white;"
    "    margin: 20px 0;"
    "}"
    ".confidence-high {"
    "    background-color: #4CAF50;"
    "    color: white;"
    "    padding: 10px;"
    "    border-radius: 10px;"
    "    text-align: center;"
    "}"
    ".confidence-medium {"
    "    background-color: #FF9800;"
    "    color: white;"
    "    padding: 10px;"
    "    border-radius: 10px;"
    "    text-align: center;"
    "}"
    ".confidence-low {"
    "    background-color: #f44336;"
    "    color: white;"
    "    padding: 10px;"
    "    border-radius: 10px;"
    "    text-align: center;"
    "}"
    ".feature-box {"
    "    background-color: #f8f9fa;"
    "    padding: 20px;"
    "    border-radius: 10px;"
    "    border-left: 5px solid #1e88e5;"
    "    margin: 10px 0;"
    "    color: #333;"
    "}"
    ".stApp {"
    "    background-color: white;"
    "}"
    "h1, h2, h3, h4, h5, h6 {"
    "    color: #1e88e5 !important;"
    "}"
    "p, div, span {"
    "    color: #333 !important;"
    "}"
    ".stMarkdown {"
    "    color: #333 !important;"
    "}"
    "label {"
    "    color: #333 !important;"
    "}"
    ".sidebar .sidebar-content {"
    "    background-color: #f8f9fa !important;"
    "    color: #333 !important;"
    "}"
    ".stSidebar {"
    "    background-color: #f8f9fa !important;"
    "}"
    ".stSidebar h1, .stSidebar h2, .stSidebar h3, .stSidebar h4, .stSidebar h5, .stSidebar h6 {"
    "    color: #1e88e5 !important;"
    "}"
    ".stSidebar p, .stSidebar div, .stSidebar span {"
    "    color: #333 !important;"
    "}"
    ".stSidebar .stMarkdown {"
    "    color: #333 !important;"
    "}"
    "</style>",
    unsafe_allow_html=True
)

@st.cache_resource
def load_model():
    try:
        model = tf.keras.models.load_model("custom_model.h5")
        return model
    except Exception as e:
        st.error(f"‚ùå Error loading model: {e}")
        st.info("üìã Make sure 'custom_model.h5' exists in the current directory")
        return None

CLASS_NAMES = {
    0: "üê± Animals/Pets",
    1: "üå∏ Nature/Plants",
    2: "üè† Buildings/Architecture",
    3: "üöó Vehicles/Transportation",
    4: "üë• People/Faces"
}

CLASS_COLORS = {
    0: "#FF6B6B",
    1: "#4ECDC4",
    2: "#45B7D1",
    3: "#96CEB4",
    4: "#FFEAA7"
}

def preprocess_image(img):
    if img.mode != 'RGB':
        img = img.convert('RGB')

    img = img.resize((224, 224))

    img_array = image.img_to_array(img)
    img_array = img_array / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    return img_array

def predict_image(model, img_array):
    prediction = model.predict(img_array, verbose=0)
    predicted_class = np.argmax(prediction, axis=1)[0]
    confidence = np.max(prediction) * 100

    return predicted_class, confidence, prediction[0]

def main():
    st.markdown('<h1 class="main-header">ü§ñ AI Image Classifier</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Powered by Transfer Learning & MobileNetV2</p>', unsafe_allow_html=True)

    model = load_model()
    if model is None:
        st.stop()

    with st.sidebar:
        st.markdown("## üìã Instructions")
        st.markdown("""
        1. **Upload Image**: Choose JPG, PNG, or JPEG
        2. **AI Analysis**: Wait for processing
        3. **View Results**: See prediction & confidence
        """)

        st.markdown("## üîß Model Details")
        st.markdown(f"""
        - **Architecture**: MobileNetV2 + Custom Head
        - **Transfer Learning**: ‚úÖ Pre-trained on ImageNet
        - **Input Size**: 224√ó224 pixels
        - **Classes**: {len(CLASS_NAMES)} categories
        - **Parameters**: {model.count_params():,}
        """)

        st.markdown("## üéØ Categories")
        for class_id, class_name in CLASS_NAMES.items():
            color = CLASS_COLORS[class_id]
            st.markdown(f'<div style="background-color: {color}; padding: 5px; border-radius: 5px; margin: 2px 0; text-align: center; color: black; font-weight: bold;">{class_name}</div>', unsafe_allow_html=True)

    st.markdown("---")

    uploaded_file = st.file_uploader(
        "üì§ **Choose an image to classify**",
        type=["jpg", "png", "jpeg"],
        help="Upload an image and let AI analyze it!"
    )

    if uploaded_file is not None:
        col1, col2 = st.columns([1.2, 1])

        with col1:
            img = Image.open(uploaded_file)
            st.image(img, caption="üì∏ Your Uploaded Image", use_column_width=True)

            st.markdown("### üìä Image Information")
            st.markdown(f"""
            - **Filename**: {uploaded_file.name}
            - **Size**: {img.size[0]} √ó {img.size[1]} pixels
            - **Format**: {img.format}
            - **Mode**: {img.mode}
            - **File Size**: {len(uploaded_file.getvalue()) / 1024:.1f} KB
            """)

        with col2:
            st.markdown("### ü§ñ AI Analysis")

            with st.spinner("üîç Analyzing your image..."):
                img_array = preprocess_image(img)

                predicted_class, confidence, all_predictions = predict_image(model, img_array)

                predicted_name = CLASS_NAMES[predicted_class]

                st.markdown(f"""
                <div class="prediction-box">
                    <h3 style="margin: 0; text-align: center;">üéØ Prediction Result</h3>
                    <h2 style="margin: 10px 0; text-align: center;">{predicted_name}</h2>
                    <h3 style="margin: 0; text-align: center;">Confidence: {confidence:.1f}%</h3>
                </div>
                """, unsafe_allow_html=True)

                if confidence >= 80:
                    st.markdown('<div class="confidence-high">üéâ High Confidence - Excellent!</div>', unsafe_allow_html=True)
                elif confidence >= 60:
                    st.markdown('<div class="confidence-medium">‚ö†Ô∏è Moderate Confidence - Good</div>', unsafe_allow_html=True)
                else:
                    st.markdown('<div class="confidence-low">‚ùå Low Confidence - Uncertain</div>', unsafe_allow_html=True)

        st.markdown("---")
        st.markdown("### üìä Detailed Class Probabilities")

        sorted_predictions = sorted(enumerate(all_predictions), key=lambda x: x[1], reverse=True)

        for i, (class_id, prob) in enumerate(sorted_predictions):
            class_name = CLASS_NAMES[class_id]
            percentage = prob * 100

            col_name, col_bar, col_percent = st.columns([1, 2, 0.5])

            with col_name:
                if i == 0:
                    st.markdown(f"**üèÜ {class_name}**")
                else:
                    st.markdown(f"{class_name}")

            with col_bar:
                st.progress(float(prob))
                st.markdown(f"{percentage:.1f}%")

            with col_percent:
                st.markdown(f"**{percentage:.1f}%**")

    else:
        st.markdown("""
        <div class="feature-box">
            <h3>üåü Welcome to AI Image Classifier build by Sudais!</h3>
            <p>Upload any image and watch our AI analyze it in real-time. Our transfer learning model can classify images into multiple categories with high accuracy.</p>
        </div>
        """, unsafe_allow_html=True)

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("""
            <div class="feature-box">
                <h4>üöÄ Fast Processing</h4>
                <p>Get results in seconds using optimized MobileNetV2 architecture</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="feature-box">
                <h4>üéØ High Accuracy</h4>
                <p>Transfer learning from ImageNet ensures reliable predictions</p>
            </div>
            """, unsafe_allow_html=True)

        with col3:
            st.markdown("""
            <div class="feature-box">
                <h4>üì± Easy to Use</h4>
                <p>Simple drag-and-drop interface with detailed results</p>
            </div>
            """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
'''

with open('app.py', 'w') as f:
    f.write(streamlit_app_code)

print("‚úÖ Streamlit app created as 'app.py'")

# Start the Streamlit app directly
print("üöÄ Starting Streamlit app...")
!streamlit run app.py --server.port 8501 --server.headless true &>/dev/null&

# Wait for the app to start
import time
time.sleep(5)

print("üåê Your app is running!")
print("üì± Access it at: http://localhost:8501")

# Use Colab's built-in port forwarding
from google.colab.output import eval_js
print(f"üåê Public URL: {eval_js('google.colab.kernel.proxyPort(8501)')}")

print("\n" + "="*50)
print("üöÄ APP IS READY!")
print("="*50)
print("Your AI Image Classifier is now running!")
print("You can access it using the URL above.")
print("="*50)





public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

from pyngrok import ngrok

ngrok.kill()  # sab purane tunnels band karega

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

from pyngrok import ngrok

ngrok.kill()  # sab purane tunnels band karega

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

from pyngrok import ngrok

ngrok.kill()  # sab purane tunnels band karega

!streamlit run app.py --server.port 8501 &>/dev/null&

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

from pyngrok import ngrok

ngrok.kill()  # sab purane tunnels band karega

!streamlit run app.py --server.port 8501 &>/dev/null&

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

!streamlit run app.py --server.port 8501 &>/dev/null&

from pyngrok import ngrok

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

from pyngrok import ngrok

ngrok.kill()  # sab purane tunnels band karega

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

!streamlit run app.py --server.port 8501 &>/dev/null&

from pyngrok import ngrok

public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)



!streamlit run app.py --server.port 8501 --server.headless true

!streamlit run app.py --server.port 8502 --server.headless true

!pip install pyngrok
from pyngrok import ngrok
!streamlit run app.py & npx localtunnel --port 8502

!pip install pyngrok
from pyngrok import ngrok

# Streamlit app run karo
!streamlit run app.py &

# Ngrok tunnel banao
public_url = ngrok.connect(port='8501')
print("Public URL:", public_url)

!streamlit run app.py --server.port 8501 &>/dev/null&

from pyngrok import ngrok
public_url = ngrok.connect(8501)
print("Your Streamlit app is live here:", public_url)

# Step 1: Streamlit ko background me chalao
!streamlit run app.py --server.port 8501 &>/dev/null&

# Step 2: Ngrok tunnel banao
from pyngrok import ngrok
public_url = ngrok.connect(8501)
print("üîó Your Streamlit app is live here:", public_url)

!ngrok config add-authtoken 31bUeRFjOCKdrEKbw6d9WiDxIh2_4QVAhbyMBYMedzLb1Wt6n

!cat /root/.ngrok2/ngrok.yml

!streamlit run app.py --server.port 8501 &>/dev/null&

from pyngrok import ngrok, conf
config = conf.PyngrokConfig(auth_token="31bUeRFjOCKdrEKbw6d9WiDxIh2_4QVAhbyMBYMedzLb1Wt6n")
public_url = ngrok.connect(8501, pyngrok_config=config)
print("üîó Your Streamlit app is live here:", public_url)

!streamlit run app.py & npx localtunnel --port 8501

!pip install pyngrok
from pyngrok import ngrok

# Streamlit app ko run karte waqt
!streamlit run app.py --server.port 8501 &

# Ngrok tunnel banao
public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

from pyngrok import ngrok

# 8501 port pe chal raha hai Streamlit
public_url = ngrok.connect(8501)
print("Your Streamlit app is live at:", public_url)

!streamlit run app.py --server.port 8501 &>/dev/null&